{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc148239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavponnusamy/Documents/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ `quant_configs` is part of GPT2Model.__init__'s signature, but not documented. Make sure to add it to the docstring of the function in /Users/pranavponnusamy/Documents/EICTest/_transformers/src/transformers/models/gpt2/modeling_gpt2.py.\n",
      "None\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2AttentionQ(\n",
      "          (c_attn): LoraAttentionKV(\n",
      "            (layer): QuantLinear()\n",
      "            (lora_A_k): ParameterDict(\n",
      "                (test1): Parameter containing: [torch.FloatTensor of size 768x4]\n",
      "                (test2): Parameter containing: [torch.FloatTensor of size 768x4]\n",
      "            )\n",
      "            (lora_B_k): ParameterDict(\n",
      "                (test1): Parameter containing: [torch.FloatTensor of size 4x768]\n",
      "                (test2): Parameter containing: [torch.FloatTensor of size 4x768]\n",
      "            )\n",
      "            (lora_A_v): ParameterDict(\n",
      "                (test1): Parameter containing: [torch.FloatTensor of size 768x4]\n",
      "                (test2): Parameter containing: [torch.FloatTensor of size 768x4]\n",
      "            )\n",
      "            (lora_B_v): ParameterDict(\n",
      "                (test1): Parameter containing: [torch.FloatTensor of size 4x768]\n",
      "                (test2): Parameter containing: [torch.FloatTensor of size 4x768]\n",
      "            )\n",
      "          )\n",
      "          (c_proj): LoRALayer(\n",
      "            (layer): QuantLinear()\n",
      "            (lora_A): ParameterDict(\n",
      "                (test1): Parameter containing: [torch.FloatTensor of size 768x4]\n",
      "                (test2): Parameter containing: [torch.FloatTensor of size 768x4]\n",
      "            )\n",
      "            (lora_B): ParameterDict(\n",
      "                (test1): Parameter containing: [torch.FloatTensor of size 4x768]\n",
      "                (test2): Parameter containing: [torch.FloatTensor of size 4x768]\n",
      "            )\n",
      "          )\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLPQ(\n",
      "          (c_fc): LoRALayer(\n",
      "            (layer): QuantLinear()\n",
      "            (lora_A): ParameterDict(\n",
      "                (test1): Parameter containing: [torch.FloatTensor of size 768x4]\n",
      "                (test2): Parameter containing: [torch.FloatTensor of size 768x4]\n",
      "            )\n",
      "            (lora_B): ParameterDict(\n",
      "                (test1): Parameter containing: [torch.FloatTensor of size 4x3072]\n",
      "                (test2): Parameter containing: [torch.FloatTensor of size 4x3072]\n",
      "            )\n",
      "          )\n",
      "          (c_proj): LoRALayer(\n",
      "            (layer): QuantLinear()\n",
      "            (lora_A): ParameterDict(\n",
      "                (test1): Parameter containing: [torch.FloatTensor of size 3072x4]\n",
      "                (test2): Parameter containing: [torch.FloatTensor of size 3072x4]\n",
      "            )\n",
      "            (lora_B): ParameterDict(\n",
      "                (test1): Parameter containing: [torch.FloatTensor of size 4x768]\n",
      "                (test2): Parameter containing: [torch.FloatTensor of size 4x768]\n",
      "            )\n",
      "          )\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "torch.Size([1, 15])\n",
      "My name is Pranav. I am from India. I am Â aÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \n",
      "My name is Pranav. I am from India. I am \n",
      "\n",
      ".-,.â€¦â€¦\n",
      "..,â€¦â€¦â€”â€¦â€”â€¦ and-\n",
      ",â€”-â€¦-,.- T\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import QuantBlockConfig\n",
    "from utils import utils \n",
    "from _transformers.src.transformers.models.gpt2.modeling_gpt2 import GPT2MLPQ, GPT2AttentionQ\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel\n",
    "from utils import lora \n",
    "from transformers import GPT2Model\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "QUANT_CONFIGS = {i: utils.QuantBlockConfig() for i in range(0, 12)}\n",
    "\n",
    "precisions = [\"test1\", \"test2\"]\n",
    "dict_configs = {\"test1\": {i: {\"Attention_W_bit\": 16, \"Attention_A_bit\": 8, \"MLP_W_bit\": 8, \"MLP_A_bit\": 8} for i in range(12)}, \"test2\": {i: {\"Attention_W_bit\": 4, \"Attention_A_bit\": 4, \"MLP_W_bit\": 4, \"MLP_A_bit\": 32} for i in range(12)}}\n",
    "\n",
    "configs = {}\n",
    "for k, v in dict_configs.items(): \n",
    "    conf = [QuantBlockConfig.from_dict(dict_configs[k][i]) for i in range(12)]\n",
    "    quant_configs = {i: conf[i] for i in range(12)}\n",
    "    configs[k] = quant_configs\n",
    "\n",
    "utils.quantize_model(model, QUANT_CONFIGS)\n",
    "lora.apply_lora_to_model(model, precisions, r=4, alpha=1.0)\n",
    "\n",
    "utils.set_active_quant_config(QUANT_CONFIGS, configs[\"test1\"])\n",
    "lora.set_active_quant_config(\"test1\")\n",
    "\n",
    "print(model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_inputs = tokenizer([\"My name is Pranav. I am from India. I am \"], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "print(model_inputs.input_ids.shape)\n",
    "# Fix for when model.generate doesn't work: call model directly and use output ids from return value\n",
    "# If the model is a causal LM, calling it returns a ModelOutput with `logits` (not generated sequences).\n",
    "# To get generated ids, run with input_ids and use greedy decoding manually.\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=model_inputs.input_ids,\n",
    "        attention_mask=model_inputs.attention_mask,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=False,        # greedy decoding\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n",
    "\n",
    "utils.set_active_quant_config(QUANT_CONFIGS, configs[\"test2\"])\n",
    "lora.set_active_quant_config(\"test2\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=model_inputs.input_ids,\n",
    "        attention_mask=model_inputs.attention_mask,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=False,        # greedy decoding\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f0fa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1000\n",
      "Batches per epoch: 250\n"
     ]
    }
   ],
   "source": [
    "# ============ Load Dataset ============\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate for SQuAD dataset\"\"\"\n",
    "    return {\n",
    "        'context': [item['context'] for item in batch],\n",
    "        'question': [item['question'] for item in batch],\n",
    "        'answers': [item['answers']['text'][0] if item['answers']['text'] else \"\" for item in batch]\n",
    "    }\n",
    "\n",
    "ds = load_dataset(\"rajpurkar/squad\")\n",
    "train_data = ds['train'].select(range(1000))  # Use subset for testing\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, \n",
    "    batch_size=4, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e33428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_labels(tokenizer, contexts, questions, answers, max_length=512):\n",
    "    \"\"\"\n",
    "    Create input_ids and labels where only answer tokens contribute to loss.\n",
    "    Labels use -100 for masked positions (ignored by CrossEntropyLoss).\n",
    "    \"\"\"\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for context, question, answer in zip(contexts, questions, answers):\n",
    "        # Build the full prompt\n",
    "        prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "        full_text = f\"{prompt} {answer}{tokenizer.eos_token}\"\n",
    "        \n",
    "        # Tokenize prompt (context + question) separately to get its length\n",
    "        prompt_tokens = tokenizer(prompt, add_special_tokens=True)\n",
    "        prompt_length = len(prompt_tokens.input_ids)\n",
    "        \n",
    "        # Tokenize full sequence\n",
    "        full_tokens = tokenizer(\n",
    "            full_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = full_tokens.input_ids.squeeze(0)\n",
    "        attention_mask = full_tokens.attention_mask.squeeze(0)\n",
    "        \n",
    "        # Create labels: -100 for prompt, actual token ids for answer\n",
    "        labels = input_ids.clone()\n",
    "        labels[:prompt_length] = -100  # Mask context + question\n",
    "        \n",
    "        # Also mask padding tokens\n",
    "        labels[attention_mask == 0] = -100\n",
    "        \n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.stack(input_ids_list),\n",
    "        'attention_mask': torch.stack(attention_mask_list),\n",
    "        'labels': torch.stack(labels_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb4697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: test1: 5.2204 | test2: 10.0681\n",
      "Losses: test1: 4.7503 | test2: 9.8488\n",
      "Losses: test1: 3.0438 | test2: 8.5001\n",
      "Losses: test1: 4.3545 | test2: 8.3515\n",
      "Losses: test1: 4.7495 | test2: 9.0765\n",
      "Losses: test1: 3.6963 | test2: 8.6566\n",
      "Losses: test1: 3.0536 | test2: 8.5534\n",
      "Losses: test1: 3.8697 | test2: 8.9160\n",
      "Losses: test1: 3.1377 | test2: 8.3708\n",
      "Losses: test1: 1.9212 | test2: 9.4158\n",
      "Losses: test1: 2.9648 | test2: 7.8286\n",
      "Losses: test1: 2.7820 | test2: 8.4585\n",
      "Losses: test1: 3.5794 | test2: 8.4243\n",
      "Losses: test1: 3.2107 | test2: 8.3756\n",
      "Losses: test1: 2.1760 | test2: 8.8208\n",
      "Losses: test1: 2.7039 | test2: 9.0153\n",
      "Losses: test1: 3.1041 | test2: 8.1269\n",
      "Losses: test1: 2.3237 | test2: 8.8550\n",
      "Losses: test1: 2.1907 | test2: 7.3412\n"
     ]
    }
   ],
   "source": [
    "from utils import lora\n",
    "import torch\n",
    "\n",
    "# Your precision configs\n",
    "precisions = [\"test1\", \"test2\"]\n",
    "num_epochs = 1\n",
    "\n",
    "# Optional: weight each config's contribution to the loss\n",
    "loss_scale = {\n",
    "    \"test1\": 1.0,\n",
    "    \"test2\": 1.0,\n",
    "}\n",
    "\n",
    "# Get ALL LoRA parameters (for all configs)\n",
    "lora_params = [p for n, p in model.named_parameters() if 'lora_' in n]\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in lora_params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(lora_params, lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        # Prepare inputs (same for all configs)\n",
    "        batch_data = create_masked_labels(\n",
    "            tokenizer,\n",
    "            batch['context'],\n",
    "            batch['question'],\n",
    "            batch['answers']\n",
    "        )\n",
    "        input_ids = batch_data['input_ids'].to(model.device)\n",
    "        attention_mask = batch_data['attention_mask'].to(model.device)\n",
    "        labels = batch_data['labels'].to(model.device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero grads once at the start\n",
    "        \n",
    "        loss_values = {}\n",
    "        \n",
    "        # Forward + backward for EACH precision config\n",
    "        for precision in precisions:\n",
    "            utils.set_active_quant_config(QUANT_CONFIGS, configs[precision])\n",
    "            lora.set_active_quant_config(precision)\n",
    "                        \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            # Scale the loss for this config\n",
    "            loss = outputs.loss * loss_scale[precision]\n",
    "            \n",
    "            # Accumulate gradients (don't step yet!)\n",
    "            loss.backward()\n",
    "            \n",
    "            loss_values[precision] = outputs.loss.item()\n",
    "            \n",
    "            # Free memory\n",
    "            del outputs, loss\n",
    "        \n",
    "        # Gradient clipping (optional but recommended)\n",
    "        # torch.nn.utils.clip_grad_norm_(lora_params, max_norm=1.0)\n",
    "        \n",
    "        # Single optimizer step after all configs\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        loss_str = \" | \".join([f\"{p}: {v:.4f}\" for p, v in loss_values.items()])\n",
    "        print(f\"Losses: {loss_str}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
